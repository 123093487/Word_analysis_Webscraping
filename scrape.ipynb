{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('C:/Users/px/Downloads/Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://insights.blackcoffer.com/rise-of-telem...\n",
       "1      https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2      https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3      https://insights.blackcoffer.com/rise-of-telem...\n",
       "4      https://insights.blackcoffer.com/rise-of-telem...\n",
       "                             ...                        \n",
       "109    https://insights.blackcoffer.com/coronavirus-i...\n",
       "110    https://insights.blackcoffer.com/coronavirus-i...\n",
       "111    https://insights.blackcoffer.com/what-are-the-...\n",
       "112    https://insights.blackcoffer.com/marketing-dri...\n",
       "113    https://insights.blackcoffer.com/continued-dem...\n",
       "Name: URL, Length: 114, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error processing https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "HTTP Error processing https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the heading and paragraphs from an article URL\n",
    "def extract_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        heading = soup.find('h1').text.strip()\n",
    "        paragraphs = [p.text.strip() for p in soup.find_all('p')]\n",
    "        content = str(heading) + str(paragraphs)\n",
    "        return content\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error processing {url}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "content_list = []\n",
    "\n",
    "for link in df['URL']:\n",
    "    content = extract_article_content(link)\n",
    "    if content:\n",
    "        content_list.append(content)\n",
    "    else:\n",
    "        # If there was an error, append a placeholder or empty string\n",
    "        content_list.append('')\n",
    "\n",
    "# Assign the content list to the 'data' column in the DataFrame\n",
    "df['data'] = content_list\n",
    "\n",
    "# Now df['data'] will have the extracted content or empty strings for URLs with errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\px\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from textstat import flesch_kincaid_grade\n",
    "\n",
    "def calculate_scores(text):\n",
    "    if not text:\n",
    "        # If the text is empty, return 0 for all scores\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    analysis = TextBlob(text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    \n",
    "    # Calculate complexity and FOG index\n",
    "    words = nltk.word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    num_complex_words = sum(1 for word in words if len(word) > 6)\n",
    "    complex_word_percentage = (num_complex_words / num_words) * 100\n",
    "    fog_index = flesch_kincaid_grade(text)\n",
    "    \n",
    "    # Calculate average word length and syllables per word\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    avg_word_length = sum(word_lengths) / len(word_lengths)\n",
    "    total_syllables = sum(sum(1 for _ in nltk.re.findall(r'[aeiouyAEIOUY]+', word)) for word in words)\n",
    "    syllables_per_word = total_syllables / num_words\n",
    "    \n",
    "    # Count personal pronouns\n",
    "    personal_pronouns = [\"I\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "    personal_pronoun_count = sum(1 for word in words if word.lower() in personal_pronouns)\n",
    "    \n",
    "    return polarity, -polarity, polarity, subjectivity, len(nltk.sent_tokenize(text)), complex_word_percentage, fog_index, len(words) / len(nltk.sent_tokenize(text)), complex_word_percentage / 100 * num_words, num_words, syllables_per_word, personal_pronoun_count, avg_word_length\n",
    "\n",
    "# Apply the modified function to the DataFrame\n",
    "df[['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']] = df['data'].apply(lambda x: pd.Series(calculate_scores(x)))\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "#df.to_csv('updated_dataset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['data'],inplace = True , axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('C:/Users/px/Desktop/Assignment_Final.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
